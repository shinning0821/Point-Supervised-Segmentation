{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from albumentations import Flip, ShiftScaleRotate, RandomRotate90, RandomCrop\n",
    "from src.datasets import HEDataset,ConsepDataset,ConsepDataset_Fast\n",
    "import os \n",
    "import torch\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"5\"\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "torch.set_num_threads(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import random\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wzy/.conda/envs/general/lib/python3.7/site-packages/albumentations/augmentations/blur/transforms.py:185: UserWarning: blur_limit and sigma_limit minimum value can not be both equal to 0. blur_limit minimum value changed to 3.\n",
      "  \"blur_limit and sigma_limit minimum value can not be both equal to 0. \"\n"
     ]
    }
   ],
   "source": [
    "data_transform = A.Compose([A.Flip(p=0.3),\n",
    "                            A.Affine(p=0.3),\n",
    "                            A.Rotate(p=0.3),\n",
    "                            A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=15, val_shift_limit=10, p=0.3),\n",
    "                            A.GaussianBlur(3, p=0.3),\n",
    "                            A.GaussNoise(30, p=0.3)],\n",
    "                            \n",
    "                            keypoint_params=A.KeypointParams(format='yx'),\n",
    "                            additional_targets={'mask0': 'mask',\n",
    "                                                'mask1': 'mask',\n",
    "                                                'mask2': 'mask'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MoNuSegdisplay_set = ConsepDataset(data_dir = './CoNSeP',transform=data_transform,option = 'Train')\n",
    "# MoNu_0 = MoNuSegdisplay_set[1]\n",
    "# fig,axes = plt.subplots(1,3, figsize=(30,10))\n",
    "\n",
    "# axes[0].imshow(MoNu_0[\"images\"].numpy().transpose(1,2,0)) \n",
    "# axes[1].imshow(MoNu_0[\"region\"].numpy())\n",
    "# axes[2].imshow(MoNu_0[\"obj\"].numpy(),cmap='jet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob,os\n",
    "from trainval import trainval\n",
    "import matplotlib.pyplot as plt\n",
    "from haven import haven_utils as hu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment saved in ./Result/9cd2fe45a2b63c0b3d2d2865b76acfec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wzy/.conda/envs/general/lib/python3.7/site-packages/albumentations/augmentations/blur/transforms.py:185: UserWarning: blur_limit and sigma_limit minimum value can not be both equal to 0. blur_limit minimum value changed to 3.\n",
      "  \"blur_limit and sigma_limit minimum value can not be both equal to 0. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment at epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3375 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-6.2903, -6.2204, -6.2118,  ...,  4.1285,  4.1678,  4.2084],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-2.7376, -2.7264, -2.6896,  ...,  2.7354,  2.7768,  2.7889],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.4468:   0%|          | 1/3375 [00:03<3:44:24,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-14.2400, -14.2301, -14.2229,  ...,  14.3474,  14.3499,  14.4306],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-7.7668, -7.7665, -7.7662,  ...,  6.8006,  6.8043,  6.8052],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.2658:   0%|          | 2/3375 [00:05<2:08:39,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-18.0743, -18.0628, -18.0093,  ...,  13.8858,  13.8877,  13.8944],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-6.8846, -6.8803, -6.8796,  ...,  5.4046,  5.4072,  5.4097],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.1535:   0%|          | 3/3375 [00:06<1:44:39,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-17.7447, -17.7385, -17.7025,  ...,  11.7681,  11.7727,  11.7810],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-4.9678, -4.9665, -4.9652,  ...,  3.7848,  3.7982,  3.8005],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.0191:   0%|          | 4/3375 [00:07<1:26:39,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-17.3072, -17.3021, -17.2988,  ...,  11.0493,  11.0508,  11.0671],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-3.5763, -3.5731, -3.5728,  ...,  2.4148,  2.4182,  2.4185],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.9033:   0%|          | 5/3375 [00:08<1:16:11,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-11.7207, -11.7073, -11.6940,  ...,   8.2149,   8.2188,   8.2278],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-2.5541, -2.5530, -2.5490,  ...,  1.0003,  1.0052,  1.0130],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.8251:   0%|          | 6/3375 [00:09<1:10:11,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-8.0925, -8.0870, -8.0815,  ...,  5.7233,  5.7256,  5.7421],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-2.1894, -2.1868, -2.1863,  ...,  0.6505,  0.6571,  0.6655],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.7656:   0%|          | 7/3375 [00:10<1:07:15,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-7.4827, -7.4779, -7.4665,  ...,  4.9203,  4.9209,  4.9210],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-2.2461, -2.2459, -2.2457,  ...,  0.7674,  0.7732,  0.7775],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.7142:   0%|          | 8/3375 [00:11<1:04:16,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-7.7141, -7.6937, -7.6843,  ...,  4.1278,  4.1317,  4.1343],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-2.3637, -2.3631, -2.3595,  ...,  0.9109,  0.9110,  0.9116],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6732:   0%|          | 9/3375 [00:12<1:02:08,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-6.8644, -6.8609, -6.8575,  ...,  4.4877,  4.4933,  4.4934],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-2.4123, -2.4078, -2.4062,  ...,  1.0871,  1.0876,  1.0884],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6407:   0%|          | 10/3375 [00:13<1:00:35,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-5.6318, -5.6279, -5.6263,  ...,  4.2298,  4.2314,  4.2336],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-2.3584, -2.3548, -2.3548,  ...,  1.1596,  1.1623,  1.1655],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.6138:   0%|          | 11/3375 [00:14<59:27,  1.06s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-5.1345, -5.1318, -5.1140,  ...,  4.8983,  4.9012,  4.9040],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-2.3245, -2.3230, -2.3215,  ...,  1.1632,  1.1643,  1.1650],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5918:   0%|          | 12/3375 [00:15<59:03,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-4.8485, -4.8424, -4.8351,  ...,  5.5497,  5.5535,  5.5570],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-2.1818, -2.1815, -2.1814,  ...,  1.1297,  1.1310,  1.1335],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5728:   0%|          | 13/3375 [00:16<59:18,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-5.0922, -5.0785, -5.0648,  ...,  3.5414,  3.5433,  3.5440],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-2.1022, -2.0991, -2.0968,  ...,  1.0625,  1.0628,  1.0631],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5564:   0%|          | 14/3375 [00:17<58:24,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-4.1870, -4.1820, -4.1770,  ...,  4.1591,  4.1693,  4.1717],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-1.9899, -1.9881, -1.9878,  ...,  1.0097,  1.0102,  1.0136],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5418:   0%|          | 15/3375 [00:18<57:52,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-4.0558, -4.0269, -4.0013,  ...,  2.8855,  2.8864,  2.8904],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-1.9242, -1.9235, -1.9212,  ...,  0.9775,  0.9778,  0.9784],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5295:   0%|          | 16/3375 [00:19<57:39,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-4.4858, -4.4853, -4.4848,  ...,  2.5396,  2.5447,  2.5496],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-1.8630, -1.8622, -1.8613,  ...,  0.9437,  0.9451,  0.9463],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5185:   1%|          | 17/3375 [00:20<57:31,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-3.8768, -3.8716, -3.8709,  ...,  4.1672,  4.1683,  4.1695],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-1.9015, -1.9015, -1.9015,  ...,  0.9554,  0.9572,  0.9608],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5091:   1%|          | 18/3375 [00:21<57:23,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-3.1917, -3.1888, -3.1796,  ...,  2.7865,  2.7929,  2.8016],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-1.9198, -1.9194, -1.9191,  ...,  0.9893,  0.9917,  0.9923],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5001:   1%|          | 19/3375 [00:22<57:52,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-3.5584, -3.5567, -3.5492,  ...,  2.8188,  2.8194,  2.8195],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-2.0393, -2.0364, -2.0355,  ...,  1.0619,  1.0677,  1.0713],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4927:   1%|          | 20/3375 [00:24<57:54,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-4.4051, -4.4014, -4.3917,  ...,  3.6149,  3.6152,  3.6166],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-2.1032, -2.1014, -2.0983,  ...,  1.1289,  1.1360,  1.1412],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4857:   1%|          | 21/3375 [00:25<57:43,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-4.8801, -4.8763, -4.8709,  ...,  3.3316,  3.3323,  3.3338],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-2.2668, -2.2666, -2.2664,  ...,  1.2481,  1.2503,  1.2517],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4786:   1%|          | 22/3375 [00:26<58:11,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-6.0598, -6.0525, -6.0412,  ...,  3.5342,  3.5346,  3.5358],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-2.4320, -2.4310, -2.4284,  ...,  1.3779,  1.3842,  1.3857],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4725:   1%|          | 23/3375 [00:27<58:10,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-4.0127, -3.9763, -3.9755,  ...,  3.4323,  3.4332,  3.4342],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-2.5737, -2.5733, -2.5726,  ...,  1.4352,  1.4378,  1.4386],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4669:   1%|          | 24/3375 [00:28<57:51,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-5.6630, -5.6594, -5.6589,  ...,  5.6917,  5.6937,  5.6957],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-2.7150, -2.7085, -2.7051,  ...,  1.5362,  1.5370,  1.5378],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4621:   1%|          | 25/3375 [00:29<57:54,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-6.1775, -6.1749, -6.1740,  ...,  4.0798,  4.0914,  4.1064],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-2.8767, -2.8762, -2.8752,  ...,  1.6327,  1.6357,  1.6361],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4573:   1%|          | 26/3375 [00:30<58:07,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-6.0075, -6.0041, -6.0029,  ...,  4.2968,  4.2974,  4.2980],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-3.1609, -3.1598, -3.1549,  ...,  1.7258,  1.7268,  1.7283],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4529:   1%|          | 27/3375 [00:31<59:05,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-5.0095, -5.0092, -5.0091,  ...,  4.5678,  4.5680,  4.5683],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-3.3108, -3.3105, -3.3073,  ...,  1.7385,  1.7397,  1.7410],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4487:   1%|          | 28/3375 [00:32<59:11,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-5.9729, -5.9674, -5.9617,  ...,  3.7644,  3.7658,  3.7689],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-3.4534, -3.4534, -3.4517,  ...,  1.7794,  1.7795,  1.7800],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4446:   1%|          | 29/3375 [00:33<59:32,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-6.5917, -6.5888, -6.5881,  ...,  4.3424,  4.3434,  4.3456],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-3.6454, -3.6365, -3.6328,  ...,  1.9278,  1.9322,  1.9375],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4407:   1%|          | 30/3375 [00:34<58:16,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-4.4607, -4.4567, -4.4564,  ...,  3.9766,  3.9789,  3.9791],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-3.8609, -3.8536, -3.8456,  ...,  2.0131,  2.0134,  2.0144],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4371:   1%|          | 31/3375 [00:35<57:39,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-5.7383, -5.7342, -5.7338,  ...,  5.1457,  5.1467,  5.1514],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-3.7654, -3.7517, -3.7494,  ...,  1.8312,  1.8323,  1.8328],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4338:   1%|          | 32/3375 [00:36<57:13,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-5.3223, -5.3218, -5.3216,  ...,  5.1961,  5.1998,  5.2051],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-3.5816, -3.5807, -3.5755,  ...,  1.7954,  1.8016,  1.8049],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4303:   1%|          | 33/3375 [00:37<57:42,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-6.5385, -6.5274, -6.5236,  ...,  4.0297,  4.0298,  4.0312],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-3.6724, -3.6600, -3.6521,  ...,  1.9796,  1.9796,  1.9801],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4272:   1%|          | 34/3375 [00:38<57:58,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-5.1356, -5.1356, -5.1271,  ...,  5.1885,  5.1908,  5.1932],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-3.8573, -3.8376, -3.8286,  ...,  1.9505,  1.9508,  1.9511],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.4245:   1%|          | 35/3375 [00:39<58:27,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      "tensor([-6.4264, -6.4099, -6.3973,  ...,  5.9103,  5.9118,  5.9134],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n",
      "cam_output:\n",
      "tensor([-3.7341, -3.7327, -3.7314,  ...,  2.0608,  2.0640,  2.0646],\n",
      "       device='cuda:0', grad_fn=<Unique2Backward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_62155/828265994.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0msavedir_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./Result\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msavedir_base\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mtrainval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msavedir_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatadir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data114_1/wzy/MICCAI23/Point-Supervised-Segmentation/trainval.py\u001b[0m in \u001b[0;36mtrainval\u001b[0;34m(exp_dict, savedir_base, datadir, reset, num_workers)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0mtrain_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;31m# Validate and Visualize the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data114_1/wzy/MICCAI23/Point-Supervised-Segmentation/src/models/PONet.py\u001b[0m in \u001b[0;36mtrain_on_loader\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mscore_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0mtrain_meter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data114_1/wzy/MICCAI23/Point-Supervised-Segmentation/src/models/PONet.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, batch, **extras)\u001b[0m\n\u001b[1;32m    193\u001b[0m             lcfcn_loss.compute_weighted_crossentropy(logits, points, bkgs,\n\u001b[1;32m    194\u001b[0m                                                      \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlam_point\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m                                                      bkg_enable=self.bkg_enable)  +\\\n\u001b[0m\u001b[1;32m    196\u001b[0m              \u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdet_prob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mregions\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;31m#  self.lam_obj * lcfcn_loss.compute_obj_loss(logits, objs, regions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data114_1/wzy/MICCAI23/Point-Supervised-Segmentation/lcfcn/lcfcn_loss.py\u001b[0m in \u001b[0;36mcompute_weighted_crossentropy\u001b[0;34m(logits, points, bkgs, weights, bkg_enable)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_weighted_crossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbkgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbkg_enable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mprobs_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     f_loss = F.nll_loss(probs_log, points, \n\u001b[1;32m     16\u001b[0m                         ignore_index=0)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the dataset you want to train on, you can type 'TNBC' or 'MoNuSegTrainingData'\n",
    "datadir = './CoNSeP/' \n",
    "\n",
    "# the exp_dict is the configuration of the model which has already been saved. \n",
    "# It consists of the initial learning rate, max epoch, model name and optimizer\n",
    "# The saved one looks like this\n",
    "# {\n",
    "#     \"batch_size\": 1,\n",
    "\n",
    "\n",
    "\n",
    "#     \"lr\": 0.001,\n",
    "#     \"max_epoch\": 20,\n",
    "#     \"model\": {\n",
    "#         \"name\": \"penet\"\n",
    "#     },\n",
    "#     \"optimizer\": \"adam\"\n",
    "# }\n",
    "exp_dict = hu.load_json('Experiment_Exp/exp_config_mynet_1.json')\n",
    "exp_dict[\"batch_size\"] = 64\n",
    "exp_dict[\"max_epoch\"] = 1\n",
    "\n",
    "datadir = './CoNSeP/' \n",
    "random_seed = random.randint(0, 20201009)\n",
    "train_set = ConsepDataset_Fast(data_dir=datadir,\n",
    "                            n_classes=exp_dict[\"n_classes\"],\n",
    "\n",
    "                            \n",
    "                            transform=data_transform,\n",
    "                            option=\"Train\",\n",
    "                            random_seed=random_seed,\n",
    "                            obj_option=None,\n",
    "                            patch_size=256,\n",
    "                            bkg_option=None)\n",
    "# folddir_10 = glob.glob(os.path.join(datadir, '*_fold'))\n",
    "\n",
    "# for folddir in folddir_10:\n",
    "#     savedir_base = os.path.join(folddir,'Result')\n",
    "#     os.makedirs(savedir_base,exist_ok=True)\n",
    "savedir_base = \"./Result\"\n",
    "os.makedirs(savedir_base,exist_ok=True)\n",
    "trainval(exp_dict, savedir_base, datadir, reset=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing and Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Newnet:\n\tMissing key(s) in state_dict: \"detection_head.0.weight\", \"detection_head.0.bias\". \n\tUnexpected key(s) in state_dict: \"detection_head.fpn.P5_1.weight\", \"detection_head.fpn.P5_1.bias\", \"detection_head.fpn.P5_2.weight\", \"detection_head.fpn.P5_2.bias\", \"detection_head.fpn.P4_1.weight\", \"detection_head.fpn.P4_1.bias\", \"detection_head.fpn.P4_2.weight\", \"detection_head.fpn.P4_2.bias\", \"detection_head.fpn.P3_1.weight\", \"detection_head.fpn.P3_1.bias\", \"detection_head.fpn.P3_2.weight\", \"detection_head.fpn.P3_2.bias\", \"detection_head.fpn.P6.weight\", \"detection_head.fpn.P6.bias\", \"detection_head.fpn.P7_2.weight\", \"detection_head.fpn.P7_2.bias\", \"detection_head.regressionModel.conv1.weight\", \"detection_head.regressionModel.conv1.bias\", \"detection_head.regressionModel.conv2.weight\", \"detection_head.regressionModel.conv2.bias\", \"detection_head.regressionModel.conv3.weight\", \"detection_head.regressionModel.conv3.bias\", \"detection_head.regressionModel.conv4.weight\", \"detection_head.regressionModel.conv4.bias\", \"detection_head.regressionModel.output.weight\", \"detection_head.regressionModel.output.bias\", \"detection_head.classificationModel.conv1.weight\", \"detection_head.classificationModel.conv1.bias\", \"detection_head.classificationModel.conv2.weight\", \"detection_head.classificationModel.conv2.bias\", \"detection_head.classificationModel.conv3.weight\", \"detection_head.classificationModel.conv3.bias\", \"detection_head.classificationModel.conv4.weight\", \"detection_head.classificationModel.conv4.bias\", \"detection_head.classificationModel.output.weight\", \"detection_head.classificationModel.output.bias\". \n\tsize mismatch for segmentation_head.0.weight: copying a param with shape torch.Size([1, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([2, 512, 1, 1]).\n\tsize mismatch for segmentation_head.0.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([2]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30863/2392496130.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexp_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Result/v33/model_best.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/data114_1/wzy/MICCAI23/Point-Supervised-Segmentation/src/models/PONet.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"opt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/general/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1481\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1483\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1484\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Newnet:\n\tMissing key(s) in state_dict: \"detection_head.0.weight\", \"detection_head.0.bias\". \n\tUnexpected key(s) in state_dict: \"detection_head.fpn.P5_1.weight\", \"detection_head.fpn.P5_1.bias\", \"detection_head.fpn.P5_2.weight\", \"detection_head.fpn.P5_2.bias\", \"detection_head.fpn.P4_1.weight\", \"detection_head.fpn.P4_1.bias\", \"detection_head.fpn.P4_2.weight\", \"detection_head.fpn.P4_2.bias\", \"detection_head.fpn.P3_1.weight\", \"detection_head.fpn.P3_1.bias\", \"detection_head.fpn.P3_2.weight\", \"detection_head.fpn.P3_2.bias\", \"detection_head.fpn.P6.weight\", \"detection_head.fpn.P6.bias\", \"detection_head.fpn.P7_2.weight\", \"detection_head.fpn.P7_2.bias\", \"detection_head.regressionModel.conv1.weight\", \"detection_head.regressionModel.conv1.bias\", \"detection_head.regressionModel.conv2.weight\", \"detection_head.regressionModel.conv2.bias\", \"detection_head.regressionModel.conv3.weight\", \"detection_head.regressionModel.conv3.bias\", \"detection_head.regressionModel.conv4.weight\", \"detection_head.regressionModel.conv4.bias\", \"detection_head.regressionModel.output.weight\", \"detection_head.regressionModel.output.bias\", \"detection_head.classificationModel.conv1.weight\", \"detection_head.classificationModel.conv1.bias\", \"detection_head.classificationModel.conv2.weight\", \"detection_head.classificationModel.conv2.bias\", \"detection_head.classificationModel.conv3.weight\", \"detection_head.classificationModel.conv3.bias\", \"detection_head.classificationModel.conv4.weight\", \"detection_head.classificationModel.conv4.bias\", \"detection_head.classificationModel.output.weight\", \"detection_head.classificationModel.output.bias\". \n\tsize mismatch for segmentation_head.0.weight: copying a param with shape torch.Size([1, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([2, 512, 1, 1]).\n\tsize mismatch for segmentation_head.0.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([2])."
     ]
    }
   ],
   "source": [
    "from src import models\n",
    "from src.datasets import HEDataset,ConsepDataset\n",
    "# from trainval import loadsplittedfolds\n",
    "import albumentations as A\n",
    "import os,glob\n",
    "import torch\n",
    "\n",
    "datadir = './CoNSeP/' \n",
    "# folddir_10 = glob.glob(os.path.join(datadir, '*_fold'))\n",
    "\n",
    "# folddir = folddir_10[0]\n",
    "\n",
    "exp_dict = hu.load_json('Experiment_Exp/exp_config_mynet_1.json')\n",
    "\n",
    "test_transform = A.Compose([A.Resize(256,256)], \n",
    "                          keypoint_params = A.KeypointParams(format='yx'), \n",
    "                          additional_targets = {'mask0':'mask', \n",
    "                                                'mask1':'mask'})\n",
    "\n",
    "# tr_idx, val_idx, ts_idx = loadsplittedfolds(folddir)\n",
    "test_set = test_set = ConsepDataset(data_dir = datadir, \n",
    "                                transform = test_transform,\n",
    "                                option = \"Test\")\n",
    "\n",
    "model = models.get_model(exp_dict['model'],exp_dict = exp_dict,train_set = test_set)\n",
    "model.load_state_dict(torch.load('Result/v33/model_best.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 2\n",
    "test_im = test_set[index]['images'][None]\n",
    "model.eval()\n",
    "test_logit,regression,classification = model.model_base(test_im.to(model.device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分割结果的可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prob = test_logit.sigmoid()\n",
    "test_seg = torch.argmax(test_prob, dim = 1)\n",
    "fig, axes = plt.subplots(1,3, figsize = (30,10))\n",
    "axes[0].imshow(test_im[0].detach().cpu().numpy().transpose(1,2,0))\n",
    "axes[1].imshow(test_set[index]['gt'],vmax = 7, vmin = 0)\n",
    "axes[2].imshow(test_seg[0].detach().cpu().numpy(),vmax = 7, vmin = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第1,2个loss是正常的，现在怀疑第三个loss的第二部分有一定的问题，可能还是和预处理的过程有一定的关系\n",
    "- v1: 图像监督 + 点监督 + obj loss \n",
    "- v2: 图像监督 + 点监督\n",
    "- v3: 只有点监督，基本上只有按照颜色的预测\n",
    "- v4：加上了obj部分的loss，感觉得到了一定的优化，但是也不是很明显\n",
    "- v5: 加上了图片级别的监督信息（是否有细胞核存在）\n",
    "- v6：用point当做region来计算region loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('general')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a71cc4314146b0e933944bc0f68d974dbf6df44331e2b71acc3dc305d8c9a61e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
